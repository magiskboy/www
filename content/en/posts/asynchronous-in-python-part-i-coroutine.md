---
title: "Asynchronous in Python, Part I: Coroutine"
date: 2020-12-21T10:19:38+07:00
published: false
tags:
- python
- programming
- network
- asynchronous
- coroutine
categories:
- Programming
---

### Objective

* Explore and compare the models programming for asynchronous problems
* Asynchronous, use cases, and some approaches
* What are the coroutines? How does it work? Comparations units of work
* Coroutine in application

### What is asynchronous?

Follow [Wikipedia](https://en.wikipedia.org/wiki/Asynchrony_(computer_programming))
> Asynchrony, in computer programming, refers to the occurrence of events independent of the main program flow and ways to deal with such events. These may be "outside" events such as the arrival of signals, or actions instigated by a program that takes place concurrently with program execution, without the program blocking to wait for results. Asynchronous input/output is an example of the latter cause of asynchrony, and lets programs issue commands to storage or network devices that service these requests while the processor continues executing the program. Doing so provides a degree of parallelism

The below, we have some solutions for asynchronous tasks

{{< image
    url="/async-overview.jpg"
    title="Asynchronous ecosystem"
>}}

Notice, the python thread is the green thread, it is managed by interpreter instead of OS. Thus, Python threads aren't parallelism and I hate it.

We can see that threads and processes own isolated space memory thus they can independently work with the main process.

Opposite, the event loop maintains tasks, which is shared memory and we must answer the question `How do we organize memory space for independent tasks?`


### When we use the event loop, threads, and processes?

In computer science, we can classify two classes of the task:
* CPU bound
    > In computer science, a computer is CPU-bound (or compute-bound) when the time for it to complete a task is determined principally by the speed of the central processor: processor utilization is high, perhaps at 100% usage for many seconds or minutes. Interrupts generated by peripherals may be processed slowly, or indefinitely delayed.
    
* I/O bound
    > I/O bound refers to a condition in which the time it takes to complete a computation is determined principally by the period spent waiting for input/output operations to be completed. This is the opposite of a task being CPU bound. This circumstance arises when the rate at which data is requested is slower than the rate it is consumed or, in other words, more time is spent requesting data than processing it
    
**Example**:
* using multiprocess (native thread) for I/O bound

    Jason asks David that 'What do you do today?' and David responses that his tasks are pending in testing and he doesn't have tasks thus he will back home and ðŸ˜´.
    
    No, it isn't an optimization solution. Instead, David should do other tasks until Ms.Tee said that 'Hey David, your tasks failed, fix it' ðŸ˜¥.
    
    Awesome, this is how the event loop works for I/O bound tasks
    
* using event loop for CPU bound

    Our team has 5 members and 5 tasks but only David takes all tasks then do. Because each task must be committed at the end of the day so David does each task for an hour and move on to another task. Thus David was sick at the weekend ðŸ˜·.
    
    No, we have 5 members and why do we push all tasks to David? Jason can deal with the tasks to other members and end of the day, all tasks are committed but David is ok.
    
    This is how multiprocessing (native threads) works.
    
The event loop useful in the event system and related others. They can be the best solution for I/O bound problems  

#### Problems of the task in the event loop model

We knew that we have a context in the function. The context includes variables and they are unlocated by the interpreter after the return command is executed. 

In I/O problems, we usually have tasks, which consist of many small parts interrupted by commands delay for IO. At the interrupted point, __we need return control for the caller (event loop) from callee (task function) and we also need to execute this function at the interrupt__, which is the return.

:white_check_mark: __The best solution is using coroutine__

### What is coroutine?

Donald Knuth says:
> Subroutines are special cases of coroutine


How does coroutine work?

{{< image
    url="/subroutine_coroutine.png"
    title="Subroutine vs Coroutine"
>}}

__Why coroutine useful for event system?__

* is non-preemptive scheduling
* can suspend and resume at the anywhere so if data is the stream, they can save memory
* can maintain state
* for I/O bound, coroutine optimize memory and CPU
* they are small

#### Unit of works

|                            | Process | Native thread | Green thread | Goroutine | Coroutine |
| :------------------------: | :-----: | :-----------: | :----------: | :-------: | :-------: |
|         __Memory__         |  â‰¤ 8Mb  |    â‰¤ Nx2Mb    |    â‰¥ 64Kb    |   â‰¥ 8Kb   |   â‰¥ 0Mb   |
|       __OS managed__       |   Yes   |      Yes      |      No      |    No     |    No     |
| __Pre-emptive scheduling__ |   Yes   |      Yes      |     Yes      |    No     |    No     |
| __Private address space__  |   Yes   |      No       |      No      |    No     |    No     |
|        __Parallel__        |   Yes   |      Yes      |      No      |    Yes    |    No     |


#### How to implement coroutine from scratch?

```c
#include <stdio.h>

int coroutine() {
    static int i = 0, s = 0;
    switch (s) {
        case 0:
            for (i = 0;; ++i) {
                if (!s) s = 1;
                return i;
                case 1:;
            }
    }
}

int main(int argc, char** argv) {
    printf("%d\n", coroutine());     // ?
    printf("%d\n", coroutine());     // ?
    printf("%d\n", coroutine());     // ?
    return 0;
}

```

Basically, it tries to save the subroutine's state in the `i` variable (this is a breakpoint) and when it resumes subroutine at exited point (in this case, it's `case 1:;`) and it continues to run for-loop.

In this code, the key point is s variable and how to code resume and suspend coroutine by using the `switch case` statement.

And the below, it's converted to Python code from above C code

```python
def coroutine():
    i = 0
    while 1:
        yield i
        i += 1

co = coroutine()
next(co)
next(co)
next(co)
```


#### Can you convert Python code to C?

```python
def fib():
    a, b = 0, 1
    while True:
        yield a
        a, b = a + b, a

co = fib()
for _ in range(10):
    print(next(co), end=' ')
```

Then you should see that
```
0 1 1 2 3 5 8 13 21 34
```

I can build any coroutine in C code. Can you do that?

```c
#include <stdio.h>

int fib() {
    static int i, __resume__ = 0;
    static a = 0, b = 1, c;
    switch (__resume__) {
        case 0:
            for (i = 0;; ++i) {
                if (!__resume__) __resume__ = 1;
                c = a + b;
                b = a;
                a = c;
                return a;
                case 1:;
            }
    }
}

int main() {
    for (int i = 0; i < 10; ++i) {
        printf("%d ", fib());
    }
    return 0;
}
```

```python
def say():
    yield "C"
    yield "Java"
    yield "Python"
    
co = say()
print(next(co))
print(next(co))
print(next(co))
print(next(co))
```

Then the result is 

```python
C
Java
Python
---------------------------------------------------------------------------
StopIteration                             Traceback (most recent call last)
<ipython-input-1-913b1d7d4200> in <module>
      8 print(next(co))
      9 print(next(co))
---> 10 print(next(co))

StopIteration:
```

```c
#include <stdio.h>

char* say() {
    static int __resume__ = 0;
    switch (__resume__) {
        case 0:
            __resume__ = 1;
            return "C";
        case 1:
            __resume__ = 2;
            return "Java";
        case 2:
            __resume__ = 3;
            return "Python";
        default:
            return NULL;           // GeneratorExit
    }
}

int main() {
    printf("%s\n", say());
    printf("%s\n", say());
    printf("%s\n", say());
    printf("%s\n", say());
    return 0;
}

```

We can see that coroutine needs a static memory namespace to save context when it suspends and resumes without lost context. In C, the static namespace is static variables, they are maintained by OS when the function is done. In Python, the context of the function is stored in the stack frame. 

Let's think about coroutines are segments of the program, not have private memory, not have parallel and very safe

{{< image
    url="/co-thread.png"
    title="Coroutine vs Threads"
>}}


Coroutine increases many bugs of multiprocessing and I think it is the best solution for networking tasks because it is alive in a single process.

In Python, we can define coroutine by use the` yield` statement in the function. When we call functions, they return a coroutine object instead of a final value

```python
def coro_fn():
    val = yield 'Starting'   # started coroutine and suspend, return control to caller
    print('Consume', val)
    yield 'Hello World'      # produce data
    
co = coro_fn()               # create a new coroutine object
print(co.send(None))         # start coroutine
print(co.send('data'))       # resume coroutine, pass control into coroutine
co.close()                   # close coroutine
```

Then it's the result

```python
Starting
Consume data
Hello World
```

The generator is the special coroutine, they can only produce data without consuming data

```python
def g1():
    for i in range(10):
        yield i
        
def g2():
    for i in range(10, 20):
        yield i
```

```python
def g():
    for i in g1():
        yield i
    for i in g2():
        yield i

list(g())
```

And it's result
```python
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
```

We can refactor this code with `yield from`

```python
def g():
    yield from g1()
    yield from g2()
    
list(g())
```

#### Build binary tree with `yield from`

```python
class Node:
    def __init__(self, value=None, left_nodes=None, right_node=None):
        self.left_nodes = left_nodes or []
        self.right_nodes = right_node or []
        self.value = value
        
    def visit(self):
        for node in self.left_nodes:
            yield from node.visit()
        yield self.value
        for node in self.right_nodes:
            yield from node.visit()
            
root = Node(
    0,
    [Node(1, [Node(7), Node(8)]), Node(2, None, [Node(9), Node(10)]), Node(3)],
    [Node(4), Node(5), Node(6)]
)
for value in root.visit():
    print(value, end=' ')
```

```
7 8 1 2 9 10 3 0 4 5 6
```

### Coroutine in application


#### 1. Asynchronous TCP server

In this case, a TCP server is an event system:

* event source: the listening socket and the connection sockets

* basically, we have two types of events: __EVENT_READ__, __EVENT_WRITE__

* and tasks are coroutines, each task will handle a connection and an event at a time

* We also have an event loop, it is I/O multiplexing for the file descriptor


```python
import logging
from sys import stdout
from socket import socket, SOCK_STREAM, AF_INET
from selectors import DefaultSelector, EVENT_READ, EVENT_WRITE

logging.basicConfig(stream=stdout, level=logging.DEBUG)

class Server:
    def __init__(self, host, port, buf_size=64):
        self.addr = (host, port)
        self.poll = DefaultSelector()
        self.m = {}
        self.buf_size = buf_size

    def handle_read(self, sock):  # make isolated environment for each connection
        buffer_size = self.buf_size
        handle_write = self.handle_write

        def _can_read():
            chunks = []
            while 1:
                chunk = sock.recv(buffer_size)
                if chunk.endswith(b'\n\n'):
                    chunks.append(chunk[:-2])
                    break
                else:
                    chunks.append(chunk)
                    yield

            handle_write(sock, b''.join(chunks))

        handler = _can_read()
        self.m[sock] = handler
        self.poll.register(sock, EVENT_READ, handler)

    def handle_write(self, sock, data):     # make isolated environment for each connection
        poll = self.poll
        m = self.m
        buffer_size = self.buf_size

        def _can_write():
            nonlocal data, sock
            start_, end_ = 0, 0
            data = b'Hello ' + data
            len_data = len(data)

            while 1:
                end_ = min(start_ + buffer_size, len_data)
                if start_ >= end_:
                    break
                sock.send(data[start_:end_])
                start_ += buffer_size
                yield    # this is susppend comamnd

            # clear handler of connection and close conection
            poll.unregister(sock)
            del m[sock]
            sock.close()
            del sock

        handler = _can_write()
        m[sock] = handler
        poll.modify(sock, EVENT_WRITE, handler)

    def handle_accept(self, sock):
        while 1:
            s, addr = sock.accept()
            logging.debug(f'Accept the connection from {addr}')
            self.handle_read(s)
            yield

    def mainloop(self):
        try:
            sock = socket(AF_INET, SOCK_STREAM)
            sock.bind(self.addr)
            sock.setblocking(0)
            sock.listen(1024)

            self.m[sock] = self.handle_accept(sock)
            self.poll.register(sock, EVENT_READ, self.m[sock])

            logging.info(f'Server is running at {self.addr}')
            while 1:
                events = self.poll.select()
                for event, _ in events:
                    try:
                        cb = event.data
                        next(cb)
                    except StopIteration:
                        pass
        except Exception as e:
            sock.close()
            self.poll.close()
            raise e
```

You can run it

```python
server = Server('127.0.0.1', 5000)
server.mainloop()
```

Scheduler and task in real: https://github.com/dabeaz/curio/blob/master/curio/kernel.py#L188

### 2. Streaming system

We can use coroutines to build up a data processing system. Basically, the system was separate from logic blocks. They placed in coroutines with owned context. You can see in the below picture.

{{< image
    url="/simple-data-processing.png"
    title="Data processing model"
>}}

Event sources are Redis pub/sub, Kafka, RabbitMQ, user interactive, and so on.

We can describe any kind of system if we create a specific logic block: filter, conditional, selector, broadcast,...

Example: build statistic IP from access log in the webserver

First, you need the [log data file](https://firebasestorage.googleapis.com/v0/b/myblog-e552f.appspot.com/o/asynchronous-in-python-part-i-coroutine%2Faccess.log?alt=media&token=54c8e7de-db81-49ed-a0b8-f3dfa05c5d24)

{{< image
    url="/IP-Statistic-v2.png"
    title="Use coroutine in the thread"
>}}


```python
def coroutine(f):
    def decorator(*args, **kwargs):
        co = f(*args, **kwargs)
        co.send(None)   # start coroutine before it's used
        return co
    return decorator
 
@coroutine
def broadcast(targets):
    try:
        while 1:
            data = yield
            for target in targets:
                target.send(data)
    except GeneratorExit:
        for target in targets:
            target.close()
            
@coroutine
def map_(ip, next_):
    try:
        while 1:
            data = yield
            if data.startswith(ip):
                next_.send(ip)
    except GeneratorExit:
        next_.close()
        
@coroutine
def reduce_(on_done):
    m = {}
    try:
        while 1:
            data = yield
            if data not in m:
                m[data] = 1
            else:
                m[data] += 1
    except GeneratorExit:
        on_done(m)
```

You can run

```python
result = {}
def on_done(r):
    global result
    result = r

reducer = reduce_(on_done)
flow = broadcast([
    map_('83.149.9.216', reducer),
    map_('93.114.45.13', reducer),
    map_('207.241.237.101', reducer),
])

# this is the source data
# We have 10000 lines in this log
%time
with open('assets/files/access.log', 'r') as fp:
    for line in fp.readlines():
        flow.send(line)
    flow.close()

print(result)
```

And it's the result

```
CPU times: user 2 Âµs, sys: 1e+03 ns, total: 3 Âµs
Wall time: 5.25 Âµs
{'83.149.9.216': 23, '93.114.45.13': 6, '207.241.237.101': 17}
```

#### Improvement

We can wrapper threads (or process or machine) in coroutines, Why not?

Simplify, I use threads instead of machine

OK, let's redesign the above diagram


{{< image
    url="/IP-Statistic-v2.png"
    title="Combine between coroutine and thread"
>}}
In the above diagram, I move logic into threads and I use queues as communication channels with threads.

Furthermore, queues are used as a buffer if the rate of input is greater than the rate of output.

```python
from threading import Thread
from queue import Queue

def coroutine(f):
    def decorator(*args, **kwargs):
        co = f(*args, **kwargs)
        co.send(None)
        return co
    return decorator
 
@coroutine
def broadcast_threaded(targets):
    queue = Queue()
    def _run_target():
        nonlocal queue, targets
        while 1:
            data = queue.get()
            if data is GeneratorExit:
                for target in targets:
                    target.close()
                return
            else:
                for target in targets:
                    target.send(data)
    Thread(target=_run_target).start()
    try:
        while 1:
            data = yield
            queue.put(data)
    except GeneratorExit:
        queue.put(GeneratorExit)
            
@coroutine
def map_threaded(ip, next_):
    queue = Queue()
    def _run_target():
        nonlocal ip, queue
        while 1:
            data = queue.get()
            if data is GeneratorExit:
                next_.close()
                return
            else:
                if data.startswith(ip):
                    while next_.gi_running:
                        pass
                    next_.send(ip)
                    queue.task_done()
    Thread(target=_run_target).start()
    try:
        while 1:
            data = yield
            queue.put(data)
    except GeneratorExit:
        queue.put(GeneratorExit)
        
@coroutine
def reduce_threaded(on_done):
    m = {}
    queue = Queue()
    def _run_target():
        nonlocal queue, m, on_done
        while 1:
            data = queue.get()
            if data is GeneratorExit:
                on_done(m)
                return
            else:
                if data not in m:
                    m[data] = 1
                else:
                    m[data] += 1
    Thread(target=_run_target).start()
    try:
        while 1:
            data = yield
            queue.put(data)
    except GeneratorExit:
        queue.put(GeneratorExit)
```

And run

```python
result = {}
def on_done(r):
    global result
    result = r

reducer = reduce_threaded(on_done)
flow = broadcast_threaded([
    map_threaded('83.149.9.216', reducer),
    map_threaded('93.114.45.13', reducer),
    map_threaded('207.241.237.101', reducer),
])

# this is the source data
# We have 10000 lines in this log
%time
with open('assets/files/access.log', 'r') as fp:
    for line in fp.readlines():
        flow.send(line)
    flow.close()

print(result)         # result?
```

Then

```sh
CPU times: user 2 Âµs, sys: 0 ns, total: 2 Âµs
Wall time: 5.72 Âµs
{}
```

Oh, why the result is empty?

I give a hint to you, let's add the `sleep` function before `print` and run again, you will understand it :smile:

__Notice__

* When we use another unit of work, which not the same as the flow of coroutines, we should consider coroutine is overloaded. At this time, the coroutine is processing data then sources push data to this coroutine. That's dangerous  

* Don't design cyclic models

* Only call `send()` in synchronize flow, I mean only call `send()` method in a single thread

### 3. Scheduler for OS


{{< image
    url="/os-scheduler.png"
    title="Operation system scheduler"
>}}

When a statement hits a trap, the program passes control to OS and OS executes the statements or switch other tasks and passes control to it.

But this model is a non-preemptive scheduler, I give you this model to explain the relationship between `yield` expression and `trap` in OS


```python
from queue import Queue

class SystemCall:
    __slots__ = ('sched', 'target')

    def handle(self):
        pass

class Task:
    __slots__ = ('id', 'target', 'sendval')
    _id = 0

    def __init__(self, target):
        Task._id += 1
        self.id = Task._id
        self.target = target
        self.sendval = None

    def run(self):
        return self.target.send(self.sendval)

class Scheduler:
    __slots__ = ('taskmap', 'ready')

    def __init__(self):
        self.taskmap = {}
        self.ready = Queue()

    def new(self, target):
        task = Task(target)
        self.taskmap[task.id] = task
        self.schedule(task)
        return task.id

    def mainloop(self):
        while self.taskmap:
            task = self.ready.get()
            try:
                result = task.run()
                if isinstance(result, SystemCall):
                    result.task = task
                    result.sched = self
                    result.handle()
                    continue
            except StopIteration:
                self.exit(task)
            else:
                self.schedule(task)

    def schedule(self, task):
        self.ready.put(task)

    def exit(self, task):
        print('Task %d terminated' % task.id)
        del self.taskmap[task.id]

class GetTid(SystemCall):
    def handle(self):
        self.task.sendval = self.task.id
        self.sched.schedule(self.task)

class NewTask(SystemCall):
    def __init__(self, target):
        self.target = target

    def handle(self):
        tid = self.sched.new(self.target)
        self.task.sendval = tid
        self.sched.schedule(self.task)

class KillTask(SystemCall):
    def __init__(self, tid):
        self.tid = tid

    def handle(self):
        task = self.sched.taskmap.get(self.tid, None)
        if task:
            task.target.close()
            self.task.sendval = True
        else:
            self.task.sendval = False
        self.sched.schedule(self.task)
```

OK, run OS

```python
def foo():
    tid = yield GetTid()
    print(f'I\'m foo and I am living in {tid} process')
    for i in range(5):
        print(f"Foo {tid} is in {i} step")
        yield

def bar():
    tid = yield GetTid()
    print(f"I'm bar and I'm living in {tid} process")
    yield NewTask(foo())
    for i in range(3):
        print(f"Bar {tid} is in {i} step")
        yield
    yield KillTask(1)

sched = Scheduler()
sched.new(foo())
sched.new(bar())
sched.mainloop()
```

And you can see

```
I'm foo and I am living in 1 process
Foo 1 is in 0 step
I'm bar and I'm living in 2 process
Foo 1 is in 1 step
Bar 2 is in 0 step
Foo 1 is in 2 step
I'm foo and I am living in 3 process
Foo 3 is in 0 step
Bar 2 is in 1 step
Foo 1 is in 3 step
Foo 3 is in 1 step
Bar 2 is in 2 step
Foo 1 is in 4 step
Foo 3 is in 2 step
Task 1 terminated
Foo 3 is in 3 step
Task 2 terminated
Foo 3 is in 4 step
Task 3 terminated
```

That's awesome!

Thank you for reading:smile:


### References:

* Python documentation https://docs.python.org/3/
* Talk about coroutine https://www.dabeaz.com/coroutines/Coroutines.pdf
