---
title: "Human in the Loop with LangGraph"
date: 2025-05-12T22:29:00+07:00
published: true
score: 1
tags:
  - ai
  - langchain
  - langgraph
  - llm
categories:
  - AI
author: "Nguy·ªÖn Kh·∫Øc Th√†nh"
description: |
    In an era where Large Language Models (LLMs) are becoming increasingly widespread, the ecosystem of tools surrounding them is also rapidly evolving. While LLMs are capable of impressive automation, they don‚Äôt always make fully accurate decisions‚Äîespecially when it comes to high-stakes tasks or those requiring a high degree of reliability.

    Human-in-the-Loop (HITL) is a technique that allows humans to intervene in an AI system‚Äôs processing pipeline‚Äîconfirming, editing, or providing additional information when necessary. With LangGraph, you can use the interrupt() function to pause execution at predefined checkpoints and wait for human feedback before proceeding. Once input like ‚Äúaccept‚Äù or ‚Äúedit‚Äù is received, the process can resume using Command(resume=...), transitioning the state forward.

    This technique is already being applied in systems like Claude Desktop and Cursor Editor, where it helps ensure that all critical actions and content are subject to human oversight.

    In this article, I‚Äôll walk you through how to implement Human-in-the-Loop using LangGraph (for logic flow and checkpoints) and Chainlit (to build a manual interaction interface). The goal is to help you establish a clear, moderated AI workflow‚Äîenabling efficient human control and feedback.
---

## Chu·∫©n b·ªã

Before getting started, you need to set up the necessary libraries and understand the role of each component in the project:


__1. LangChain__


A powerful framework for building applications with LLMs, LangChain provides standardized interfaces for RAG, Chains, Agents, Memory, and integrates seamlessly with various vector stores, LLMs, and more.

__2. LangGraph__

An extension library built on top of LangChain, LangGraph enables the construction of complex AI agents through a graph-based architecture. It supports stateful flows, loops, branching logic, and notably, streaming and human-in-the-loop interactions.

__3. Chainlit__

A Python library for designing chat interfaces for LLMs‚ÄîChainlit operates on an event-driven architecture with an API similar to Discord. With Chainlit, you can build interactive UIs without needing any frontend knowledge.

__4. C√°c th∆∞ vi·ªán h·ªó tr·ª£ kh√°c__

	‚Ä¢	langchain-google-genai: enables integration with Google‚Äôs Gemini models.

	‚Ä¢	langchain-mcp-adapters: supports the Model Context Protocol (MCP) within LangGraph.


Some important notes:


	‚Ä¢	LangGraph operates by defining nodes and edges within a graph. All data in the application flows through this graph, which also emits events (e.g., LLM messages, tool outputs) to connect with other components.


	‚Ä¢	Chainlit is an event-driven framework with a handling style similar to the Discord API. If you‚Äôre familiar with the Discord API, Chainlit will feel intuitive. Its parallel event handling also complements LangGraph‚Äôs architecture well.


With these tools in place, you‚Äôre ready to set up a complete Human-in-the-Loop workflow‚Äîfrom backend logic to frontend interface. Before diving into implementation details, make sure you‚Äôve installed all necessary components and have a basic understanding of each one‚Äôs functionality.


## Thi·∫øt k·∫ø graph

We‚Äôll design an agent capable of using tools to interact with a Kubernetes cluster via the API server. In this setup, we‚Äôll be using K8s MCP (Model Context Protocol for Kubernetes).

Now, let‚Äôs design a graph to handle user prompts.

<Image
  src="/images/hil-tut-1.png"
/>

As you can see, the user prompt begins processing at the call_model node, where the model can invoke tools if needed via the tool_node.

Once tool usage is no longer required, the message is routed to the final node for final processing before being returned to the user. At this stage, the response is rewritten in the voice and tone of Sid, the sloth character from Ice Age.

Let‚Äôs start by importing the necessary modules:

```python
import os
from typing import cast, Callable, List, Literal, Optional

import chainlit as cl
from langchain.schema.runnable.config import RunnableConfig
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.graph.graph import CompiledGraph
from langgraph.types import Command
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import MessagesState
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.types import interrupt
```

Diving into the code, we need to convert MCP calls into standard tool calls so they can be used within the LangChain framework.

```python
mcp_client = MultiServerMCPClient({
    "kubectl": {
        "command": "npx",
        "args": ["mcp-server-kubernetes"],
        "transport": "stdio"
    },
})
tools = await mcp_client.get_tools()
```

Next, we‚Äôll initialize the model and bind the tools to it so the model knows how to invoke them during execution.

```python
gemini = ChatGoogleGenerativeAI(model="gemini-2.0-flash")
model = gemini.bind_tools(tools)
```

Next, we‚Äôll define the nodes as illustrated in the diagram.

```python
def get_chat_agent():
    # kh·ªüi t·∫°o model v√† tools ·ªü tr√™n
    ...

    def call_model(state: MessagesState):
        nonlocal model
        messages = state["messages"]
        response = model.invoke(messages)
        return {"messages": [response]}


    def call_final_model(state: MessagesState):
        nonlocal model
        messages = state["messages"]
        last_ai_message = messages[-1]
        response = model.invoke(
            [
                SystemMessage("Rewrite this in the voice of Sid in Ice Age"),
                HumanMessage(last_ai_message.content),
            ]
        )
        response.id = last_ai_message.id
        return {"messages": [response]}


    def should_continue(state: MessagesState) -> Literal["tools", "final"]:
        messages = state["messages"]
        last_message = messages[-1]
        if last_message.tool_calls:
            permit = interrupt(f"I need to call **{last_message.tool_calls[0]['name']}**. Are you sure you want to call a tool?")
            if permit:
                return "tools"
            else:
                return "final"
            
        return "final"
```

Take a look at the should_continue node‚Äîthis acts as a routing node. If the model wants to perform a tool call, it must first get user permission. In LangGraph, there‚Äôs a concept called interrupt, which allows the graph (callee) to suspend execution and return control to the caller (typically, the application). The permission logic is then handled on the caller side, and once resolved, the graph can resume execution. It‚Äôs quite similar to how interrupts work in operating systems, right? üòÑ

When the external application resumes the flow, it passes a value into the permit variable, which lets the graph decide the next action based on user input.

Keep in mind that `return` or `interrupt` statements inside a node will emit an event back to the outside application.

Alright, once all the nodes are defined, we‚Äôll stitch them together into a complete graph.

```python
def get_chat_agent():
    # kh·ªüi t·∫°o model v√† tools ·ªü tr√™n
    ...
    # ƒë·ªãnh nghƒ©a c√°c node
    ...

    tool_node = ToolNode(tools=tools)

    builder = StateGraph(MessagesState)

    builder.add_node("call_model", call_model)
    builder.add_node("tools", tool_node)
    builder.add_node("final", call_final_model)

    builder.add_edge(START, "call_model")
    builder.add_conditional_edges(
        "call_model",
        should_continue,
    )

    builder.add_edge("tools", "call_model")
    builder.add_edge("final", END)

    graph = builder.compile(checkpointer=InMemorySaver())

    return graph

```

With the brain of the application now complete, we can move on to the interface layer. For this, we‚Äôll use Chainlit to design the frontend and handle user interactions.

## Let‚Äôs design the interface of the application using Chainlit.

Chainlit provides several core event handlers that form the backbone of the application lifecycle:


	‚Ä¢	on_app_startup: Triggered when the app first launches. Ideal for loading models, tools, or configs.


	‚Ä¢	on_chat_start: Called when a new chat thread begins. You can use this to initialize session state or send a welcome message.


	‚Ä¢	on_message: Invoked whenever a user submits a message in the thread. This is where the main agent interaction typically happens.


In addition to these handlers, Chainlit maintains a session for each chat thread. This session persists data across messages and allows you to store and retrieve state as needed.

You can access and manipulate session data using the cl.user_session API:

```python
@cl.on_chat_start
async def on_chat_start():
    cl.user_session.set("messages", [])
    agent = await get_chat_agent()
    cl.user_session.set("agent", agent)
```

Next, let‚Äôs implement the message handling logic with on_message in Chainlit.


```python
@cl.on_message
async def on_message(message: cl.Message):
    agent = cast(CompiledGraph, cl.user_session.get("agent"))
    thread_id = cl.context.session.id
    config = RunnableConfig(configurable={"thread_id": thread_id}, callbacks=[cl.LangchainCallbackHandler()])

    messages = cl.user_session.get("messages")
    messages.append(HumanMessage(content=message.content))
    cl.user_session.set("messages", messages)
```

The `on_message` function takes in the message sent by the user.

First, we retrieve the previously set up agent and initialize the configuration for the graph. Note that the graph can have multiple states, which are distinguished using the thread_id within this config.
This ensures that the chat history and execution context remain isolated between different threads.

Next, we store the user‚Äôs message in the session, allowing the agent to access message history in future interactions.

Then, we forward the user‚Äôs message to the graph for processing.


```python
@cl.on_message
async def on_message(message: cl.Message):
    ...

    interrupt = None
    response = cl.Message(content="")

    stream = agent.astream(
        {"messages": messages},
        config=config,
        stream_mode=['messages', 'updates'],
    )
```

We initialize a streaming LLM execution to process the message.
The `interrupt` variable is used to capture any interruptions that occur during execution‚Äîthese interruptions represent points where the LLM needs to request user permission before proceeding.
This variable holds the information passed from the `interrupt()` call we previously defined in the graph node:

```python
interrupt(f"I need to call **{last_message.tool_calls[0]['name']}**. Are you sure you want to call a tool?")
```

Now, let‚Äôs walk through the main flow, as illustrated in the diagram below.


<Image 
    src="/images/hil-flow.jpg"
/>

We‚Äôll create a stream to send the input to the LLM and then enter a loop to handle the subsequent events emitted by the graph.

If we encounter an interrupt event, we prompt the user for permission and create a corresponding Command message. This command is then sent back to the graph via another stream to resume execution.

We repeat this process until no more interruptions occur. At that point, the final message returned from the graph is shown to the user‚Äîmarking the end of that turn in the conversation.

Take a look at the code snippet below and think through how the flow is handled step by step.

```python
@cl.on_message
async def on_message(message: cl.Message):
    ...

    while stream:
        async for stream_mode, pack in stream:
            if stream_mode == 'messages':
                msg, metadata = pack
                if (
                    msg.content
                    and not isinstance(msg, HumanMessage)
                    and metadata["langgraph_node"] == "final"
                ):
                    await response.stream_token(msg.content)
                stream = None

            else:
                if '__interrupt__' in pack:
                    interrupt = pack['__interrupt__'][0]
                    res = await cl.AskActionMessage(
                        content=interrupt.value,
                        actions=[
                            cl.Action(name="continue", payload={"value": "continue"}, label="Continue"),
                            cl.Action(name="cancel", payload={"value": "cancel"}, label="Cancel"),
                        ],
                    ).send()
                    
                    if res['payload']['value'] == 'continue':
                        cmd = Command(resume=True)
                    else:
                        cmd = Command(update={"messages": [HumanMessage("I don't want to call a tool")]}, resume=False)

                    stream = agent.astream(
                        cmd,
                        config=config,
                        stream_mode=['messages', 'updates'],
                    )
                else:
                    stream = None
```

You can check out the full source code here. [https://github.com/magiskboy/hil-in-langgraph](https://github.com/magiskboy/hil-in-langgraph)


Through this article, I‚Äôve introduced a simple yet powerful technique for LLM-based applications. Beyond permission control, Human-in-the-Loop can also be used to improve LLM-generated outputs or to debug applications more effectively.

Thanks for taking the time to read!